{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure Keras is installed\n",
    "import sys\n",
    "!{sys.executable} -m pip install keras numpy > /dev/null\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this example we create a classifier to recognize handwritten digits from the famous mnist dataset, using a convolutional neural network. Typically the training data is referred to as **X**, the training lables as **y**, and the test data and labesl as **X_test** and **y_test** respectively. Let's load the train/test data, and show one of the images with the help of the excellent matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1190744f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras.datasets as datasets\n",
    "\n",
    "(X, y),  (X_test, y_test) = datasets.mnist.load_data()\n",
    "digit = X[2]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the network using convolutional layers. In this instance we used 3 convolutional layers with max pooling layers in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "net = models.Sequential()\n",
    "net.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "net.add(layers.MaxPool2D(2, 2))\n",
    "net.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "net.add(layers.MaxPool2D(2, 2))\n",
    "net.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the number each image represents we use a couple of dense layers, but before we need to flatten the matrix coming from the last convolutional layer into a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add(layers.Flatten())\n",
    "net.add(layers.Dense(64, activation='relu'))\n",
    "net.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's compile the network with an appropriate optimizer, loss and performance metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train a model, but before we need to reshape and normalize the images, and apply **one-hot-encoding** to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Reshape and normalize the images\n",
    "X = X.reshape((len(X), 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((len(X_test), 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# on-hot-encode the labels\n",
    "y = to_categorical(y)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 29s 484us/step - loss: 0.1769 - acc: 0.9445\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 29s 483us/step - loss: 0.0497 - acc: 0.9847\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 29s 481us/step - loss: 0.0337 - acc: 0.9894\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 29s 483us/step - loss: 0.0260 - acc: 0.9919\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 29s 484us/step - loss: 0.0210 - acc: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11906f3898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model performs on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 173us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03266568964242761, 0.9897]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember that in our previous notebook we obtained a test accurary of $0.977$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
