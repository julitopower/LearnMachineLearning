{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "This notebooke provides a very gentle introduction to PyTorch. We will explore tensor creation, network definition and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 4.5.12\r\n",
      "  latest version: 4.6.14\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c defaults conda\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Unncomment the following if not running in conda\n",
    "#!{sys.executable} -m pip install torch==1.0.1 seaborn==0.9.0 matplotlib numpy\n",
    "!conda install pytorch==1.0.1 seaborn==0.9.0 matplotlib numpy -y > /dev/null\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by exploring different ways of creating tensors using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.3013e-08,  3.0674e-41,  4.3066e+21],\n",
      "        [ 1.1824e+22,  4.3066e+21,  6.3828e+28],\n",
      "        [ 3.8016e-39, -3.8913e-37,  6.7262e-44]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0.1178, 0.7866, 0.2842],\n",
      "        [0.1321, 0.4596, 0.5513],\n",
      "        [0.0543, 0.6711, 0.1467]])\n",
      "tensor(3.3000)\n",
      "tensor(3.2037)\n",
      "3.2037463188171387\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors using PyTorch constructor\n",
    "data = torch.FloatTensor(3, 3)\n",
    "print(data)\n",
    "data.zero_()                     # In place\n",
    "print(data)\n",
    "\n",
    "# Creating tensors from numpy arrays\n",
    "import numpy as np\n",
    "np_data = np.random.random((3,3)).astype(np.float32)\n",
    "data = torch.tensor(np_data)\n",
    "print(data)\n",
    "\n",
    "# Scalar tensors (0 dimentional vectors, a.k.a. a point)\n",
    "scalar = torch.tensor(3.3)\n",
    "print(scalar)\n",
    "\n",
    "# We can get a scalar by summarizing a tensor\n",
    "scalar = data.sum()\n",
    "print(scalar)\n",
    "# and access the scalar value by using the special member function item()\n",
    "print(scalar.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU tensors\n",
    "\n",
    "Tensors can be backed by a CPU or GPU device. CPU tensors reside in the computer's main memory, whereas GPU tensors reside in the GPU memory. Support for GPU is restricted to cuda compatible devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CPU tensor\n",
    "data = torch.FloatTensor(3, 3)\n",
    "data.device\n",
    "\n",
    "# Create a GPU tensors: Note the following will only work on a computer with a cuda compatible GPU\n",
    "cu_data = torch.cuda.FloatTensor(3, 3)\n",
    "\n",
    "# Convert a CPU tensor to GPU tensor\n",
    "cu_data = data.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "We are going to define an expression as the following\n",
    "\n",
    "$$\n",
    "a = <a_0, a_1>\\\\\n",
    "b = <b_0, b_1>\\\\\n",
    "v_{prod} = <2 * (a_0 + b_0), 2 * (a_1 + b_1)>\\\\\n",
    "v_{res} = v_{prod_0} + v_{prod_1}\\\\\n",
    "v_{res} = (2 * (a_0 + b_0)) + (2 * (a_1 + b_1))\n",
    "$$\n",
    "\n",
    "and we will display the gradients with respect to the components of $a$ and $b$, which using simple calculus should return:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial v_{res}}{\\partial a_0} = 2, \n",
    "\\frac{\\partial v_{res}}{\\partial a_1} = 2,\n",
    "\\frac{\\partial v_{res}}{\\partial b_0} = 2, \n",
    "\\frac{\\partial v_{res}}{\\partial b_1} = 2\n",
    "$$\n",
    "\n",
    "This is all made very easy by they autograd (automatic gradient calculation) of PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.])\n",
      "tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2], dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor([2, 2], dtype=torch.float32, requires_grad=True)\n",
    "v_res = ((a + b) * 2).sum()\n",
    "v_res.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks\n",
    "\n",
    "This is the bread and butter of PyTorch day-to-day usage. Let's see how to define a neural network with three densly connected layers, using ReLu (Rectified Linear Unit) as the non-linear activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.2584, -0.6231],\n",
      "        [ 0.4604,  0.5747],\n",
      "        [ 0.4584,  0.5001]])), ('bias', tensor([-0.0767, -0.2505,  0.4687]))])\n",
      "tensor([-1.5814,  1.3593,  1.9273], grad_fn=<AddBackward0>)\n",
      "tensor([0.0000, 1.3593, 1.9273], grad_fn=<ThresholdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Linear combination operator\n",
    "linear = nn.Linear(in_features=2, out_features=3, bias=True)\n",
    "print(linear.state_dict())\n",
    "# Apply operator to some data\n",
    "data = torch.tensor([1, 2], dtype=torch.float32)\n",
    "res = linear(data)\n",
    "print(res)\n",
    "\n",
    "# ReLu operator\n",
    "relu = nn.ReLU()\n",
    "output = relu(res)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine multiple operators sequentially to build multiple layers in a simple manner, using the ```Sequential``` class. For example, the following code defines a network with 1 input layer, one hidden layer with 3 units, and one output layer with a single unit (_Note: hidden layers are normally considered to be composed of a linear operation and a non-linear activation function_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "tensor([-0.4978], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "\n",
    "print(net)\n",
    "\n",
    "res = net(data)\n",
    "res.backward()\n",
    "print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
