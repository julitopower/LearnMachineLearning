{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook provides a very simple introduction to [OpenAI Gym](https://gym.openai.com/), a toolkit for developing and comparing Reinforcement Learning (RL) algorithms. For instance, you can use OpenAI Gym to train an agent to play [Atari games!\\(https://gym.openai.com/envs/#atari)\n",
    "\n",
    "![](http://gym.openai.com/videos/2019-04-06--My9IiAbqha/SpaceInvaders-v0/poster.jpg)\n",
    "\n",
    "This notebook **is not** an introcution to RL, and does not explain concepts like Markov Decision Processes, states, rewards, value functions, policies and so on. For a hands on introduction to RL I recommend [Packt: Deep Reinforcement Learning Hands-On](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-Q-networks-ebook/dp/B076H9VQH6/ref=sr_1_1_sspa?keywords=pocket+reinforcement+learning&qid=1555782065&s=gateway&sr=8-1-spons&psc=1). For a solid theoretical treatment of the subject there is nothing better than [Sutton & Barto: Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html), which is available for free online, and can also be [purchased online](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/ref=sr_1_4?crid=17M2H3J2R3L7Z&keywords=sutton+reinforcement+learning&qid=1555782219&s=gateway&sprefix=sutton+re%2Caps%2C200&sr=8-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we can get our hands dirty there are a few things we need to install. The following cell takes care of all that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym > /dev/null\n",
    "\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the setup we need. In the following section we will introduce the foundational Gym concepts, and will execute an actual simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "OpenAI Gym is very flexible and abstracts many details to make experimentation really fast. The key to this are its foundational concepts: environment, observations, actions and spaces.\n",
    "\n",
    "**Environment**: An environment is a test problem. It models the \"world\" in which the agent exists, generates observations, defines possible actions and determines the reward the agent gets at different points in time. OpenAI Gym is packed with envirnoments. Environments are instanciated by name:\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "```\n",
    "\n",
    "**Observations**: Observations allow us to determine the state of the environment, the reward obtained after executing the last actions, wheter or not the simulation has finished, and some extra bits and pieces of information. The ```step``` method of the _environment_ gives us access to this information:\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()\n",
    "```\n",
    "\n",
    "**Actions**: The purpose of Reinforcement Learning is to learn what is the optimal action that an agent in a particular environment can take at any point in time. In OpenAI Gym the set of possible actions is defined by the environment, and can be access via its action_space. Spaces are explained next.\n",
    "\n",
    "```python\n",
    "# Select a random action\n",
    "action = env.action_space.sample()\n",
    "```\n",
    "\n",
    "**Spaces**: Every environment comes with an action_space and an observation_space. These attributes are of type Space, and they describe the format of valid actions and observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ```Discrete``` space is basically a set of actions identifed by integers. For dimension $n$ it is basically a set with elements ```range(0, n)```. A ```Box``` is basically an n-dimentional tensor. For instance, a chess board could be represented as a ```Box(8, 8)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a simulation\n",
    "\n",
    "In this section we are going to run but not visualize a simulation. We have not been able to visualize simulations within Jupyter running on EC2.\n",
    "\n",
    "We are going to use the [CartPole-v0 environment](https://gym.openai.com/envs/CartPole-v0/). CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials. The following code uses random actions, so we are not going to be able to solve the problem.\n",
    "\n",
    "The observation vector contains the following information: ```[position of cart, velocity of cart, angle of pole, rotation rate of pole]```, and the actions consist of applying a force of $-1$ or $1$ to the cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 22 timesteps. Reward 21.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 51 timesteps. Reward 50.0\n",
      "Episode finished after 43 timesteps. Reward 42.0\n",
      "Episode finished after 69 timesteps. Reward 68.0\n",
      "Episode finished after 21 timesteps. Reward 20.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 12 timesteps. Reward 11.0\n",
      "Episode finished after 22 timesteps. Reward 21.0\n",
      "Episode finished after 34 timesteps. Reward 33.0\n",
      "Episode finished after 18 timesteps. Reward 17.0\n",
      "Episode finished after 17 timesteps. Reward 16.0\n",
      "Episode finished after 30 timesteps. Reward 29.0\n",
      "Episode finished after 22 timesteps. Reward 21.0\n",
      "Episode finished after 39 timesteps. Reward 38.0\n",
      "Episode finished after 15 timesteps. Reward 14.0\n",
      "Episode finished after 39 timesteps. Reward 38.0\n",
      "Episode finished after 13 timesteps. Reward 12.0\n",
      "Episode finished after 13 timesteps. Reward 12.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 12 timesteps. Reward 11.0\n",
      "Episode finished after 15 timesteps. Reward 14.0\n",
      "Episode finished after 45 timesteps. Reward 44.0\n",
      "Episode finished after 39 timesteps. Reward 38.0\n",
      "Episode finished after 13 timesteps. Reward 12.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 24 timesteps. Reward 23.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 23 timesteps. Reward 22.0\n",
      "Episode finished after 34 timesteps. Reward 33.0\n",
      "Episode finished after 21 timesteps. Reward 20.0\n",
      "Episode finished after 21 timesteps. Reward 20.0\n",
      "Episode finished after 25 timesteps. Reward 24.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 24 timesteps. Reward 23.0\n",
      "Episode finished after 34 timesteps. Reward 33.0\n",
      "Episode finished after 31 timesteps. Reward 30.0\n",
      "Episode finished after 13 timesteps. Reward 12.0\n",
      "Episode finished after 35 timesteps. Reward 34.0\n",
      "Episode finished after 32 timesteps. Reward 31.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 37 timesteps. Reward 36.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 52 timesteps. Reward 51.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 10 timesteps. Reward 9.0\n",
      "Episode finished after 15 timesteps. Reward 14.0\n",
      "Episode finished after 15 timesteps. Reward 14.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 36 timesteps. Reward 35.0\n",
      "Episode finished after 18 timesteps. Reward 17.0\n",
      "Episode finished after 32 timesteps. Reward 31.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 18 timesteps. Reward 17.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 22 timesteps. Reward 21.0\n",
      "Episode finished after 23 timesteps. Reward 22.0\n",
      "Episode finished after 18 timesteps. Reward 17.0\n",
      "Episode finished after 26 timesteps. Reward 25.0\n",
      "Episode finished after 18 timesteps. Reward 17.0\n",
      "Episode finished after 17 timesteps. Reward 16.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 27 timesteps. Reward 26.0\n",
      "Episode finished after 38 timesteps. Reward 37.0\n",
      "Episode finished after 13 timesteps. Reward 12.0\n",
      "Episode finished after 17 timesteps. Reward 16.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 34 timesteps. Reward 33.0\n",
      "Episode finished after 12 timesteps. Reward 11.0\n",
      "Episode finished after 27 timesteps. Reward 26.0\n",
      "Episode finished after 21 timesteps. Reward 20.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 23 timesteps. Reward 22.0\n",
      "Episode finished after 11 timesteps. Reward 10.0\n",
      "Episode finished after 17 timesteps. Reward 16.0\n",
      "Episode finished after 18 timesteps. Reward 17.0\n",
      "Episode finished after 30 timesteps. Reward 29.0\n",
      "Episode finished after 27 timesteps. Reward 26.0\n",
      "Episode finished after 14 timesteps. Reward 13.0\n",
      "Episode finished after 31 timesteps. Reward 30.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 15 timesteps. Reward 14.0\n",
      "Episode finished after 11 timesteps. Reward 10.0\n",
      "Episode finished after 20 timesteps. Reward 19.0\n",
      "Episode finished after 41 timesteps. Reward 40.0\n",
      "Episode finished after 19 timesteps. Reward 18.0\n",
      "Episode finished after 16 timesteps. Reward 15.0\n",
      "Episode finished after 11 timesteps. Reward 10.0\n",
      "Episode finished after 50 timesteps. Reward 49.0\n",
      "Episode finished after 12 timesteps. Reward 11.0\n",
      "Episode finished after 20 timesteps. Reward 19.0\n",
      "Episode finished after 17 timesteps. Reward 16.0\n",
      "\n",
      "Average reward over 100 episodes is 21.55\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "episodes = 100\n",
    "total_reward = 0 # Used to calculate the average reward over all episodes\n",
    "\n",
    "def select_action(observation, env):\n",
    "    \"\"\"Select a random action\"\"\"\n",
    "    return env.action_space.sample()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    acc_reward = 0 # Accumulated reward in the current episode\n",
    "    observation = env.reset()\n",
    "    t = 0\n",
    "    while True:\n",
    "        t += 1\n",
    "        action = select_action(observation, env) # Select a random action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        acc_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps. Reward {}\".format(t + 1, acc_reward))\n",
    "            break\n",
    "    total_reward += acc_reward\n",
    "print(\"\\nAverage reward over {} episodes is {}\".format(episodes, total_reward / episodes))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our performance is far from good, which is expected from an agent that selects actions at random completely disregarding all the information available. We will work on the actual RL algorithm implementations in a different notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
